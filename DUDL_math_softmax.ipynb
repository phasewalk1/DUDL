{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMN68iFrNsMP60gW4tQAeNJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"uSjMlcaI3GfS","executionInfo":{"status":"ok","timestamp":1669753021647,"user_tz":480,"elapsed":4040,"user":{"displayName":"towardsinhale","userId":"11481387640922445900"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# Manual Softmax in Numpy"],"metadata":{"id":"WlGoCbQ2CSNZ"}},{"cell_type":"code","source":["# the scalar collection\n","z = [1,2,3]\n","\n","# numerator: e^z_i\n","num = np.exp(z)\n","# denom\n","den = np.sum( np.exp(z) )\n","sigma = num / den\n","\n","print(sigma)\n","print(f'The softmax outputs sum to: {np.sum(sigma)}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVgfM-XNCVkJ","executionInfo":{"status":"ok","timestamp":1669753197880,"user_tz":480,"elapsed":142,"user":{"displayName":"towardsinhale","userId":"11481387640922445900"}},"outputId":"04f5ef02-a84b-4b59-8ce5-af67846a4b0f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.09003057 0.24472847 0.66524096]\n","The softmax outputs sum to: 1.0\n"]}]},{"cell_type":"code","source":["# repeat with some random integers\n","z = np.random.randint(-5,high=15,size=25)\n","print(z)\n","\n","# compute the softmax result\n","num = np.exp(z)\n","den = np.sum( num )\n","sigma = num / den\n","\n","# compare\n","plt.plot(z,sigma,'ko')\n","plt.xlabel('Original number (z)')\n","plt.ylabel('Softmaxified $\\sigma$')\n","# plt.yscale('log')\n","plt.title('$\\sum\\sigma$ = %g' %np.sum(sigma))\n","plt.show()"],"metadata":{"id":"F9tIQ3t-DY9H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Softmax function in PyTorch"],"metadata":{"id":"tQLd6KJKEXyu"}},{"cell_type":"code","source":["# create an instance of the softmax activation class\n","softfun = nn.Softmax(dim=0)\n","\n","# then apply the data to that function\n","z = [1,2,3]\n","sigmaT = softfun( torch.Tensor(z) )\n","\n","print(sigmaT)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wdCYzH8EbbZ","executionInfo":{"status":"ok","timestamp":1669753749176,"user_tz":480,"elapsed":259,"user":{"displayName":"towardsinhale","userId":"11481387640922445900"}},"outputId":"dc13146d-19d0-4f4b-9a6a-e0a85034baa1"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.0900, 0.2447, 0.6652])\n"]}]}]}